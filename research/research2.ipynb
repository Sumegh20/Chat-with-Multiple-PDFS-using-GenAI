{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sumgh/Data Science/projects/Chat_with_pdf\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sumgh/Data Science/projects/Chat_with_pdf/pdf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "# from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from params import *\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documnts_from_pdf(\"/home/sumgh/Documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = \"artifacts/fassi_index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = text_splitter(documents, chunk_size=1000, chunk_overlap=200)\n",
    "create_knowledgebase(texts=text_chunks, db_path=database_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt\n",
    "template=\"\"\"You are an assistant for question-answering tasks.\n",
    "                Use the following pieces of retrieved context to answer the question.\n",
    "                If you don't know the answer, just say that you don't know.\n",
    "                Use five sentences maximum and keep the answer concise.\n",
    "                Question: {question}\n",
    "                Context: {context}\n",
    "                Chat history: {chat_history}\n",
    "                Answer:    \n",
    "            \"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### retriever\n",
    "vectordb = FAISS.load_local(folder_path=\"artifacts/fassi_index\", embeddings=load_embedding(), allow_dangerous_deserialization=True)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_handeler(input: dict):\n",
    "    return input['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chain\n",
    "rag_chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                context = input_handeler | retriever\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is my previous question?\"\n",
    "# responce = rag_chain.invoke({\"question\": question, \"chat_history\":chat_history})\n",
    "responce = rag_chain.stream({\"question\": question, \"chat_history\":chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I\n",
      " don\n",
      "'t\n",
      " know\n",
      " the\n",
      " answer\n",
      " as\n",
      " the\n",
      " previous\n",
      " question\n",
      " is\n",
      " not\n",
      " provided\n",
      " in\n",
      " the\n",
      " retrieved\n",
      " context\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for smg in responce:\n",
    "    print(smg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "throw expected at least 1 argument, got 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresponce\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: throw expected at least 1 argument, got 0"
     ]
    }
   ],
   "source": [
    "responce.()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            role=\"user\", \n",
    "            content=question\n",
    "        ),\n",
    "        AIMessage(\n",
    "            role=\"assistant\",\n",
    "            content=responce\n",
    "        ),\n",
    "    ]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What is my previous question?' role='user'\n",
      "content=\"I don't know the answer as the previous question is not provided in the retrieved context.\" role='assistant'\n",
      "content='What is my previous question?' role='user'\n",
      "content=\"I don't know the answer as the previous question is not provided in the retrieved context.\" role='assistant'\n"
     ]
    }
   ],
   "source": [
    "for chat in chat_history:\n",
    "    print(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_history.extend(\n",
    "#     [\n",
    "#         HumanMessage(\n",
    "#             role=\"user\", \n",
    "#             content=question\n",
    "#         ),\n",
    "#         AIMessage(\n",
    "#             role=\"assistant\",\n",
    "#             content=responce\n",
    "#         ),\n",
    "#     ]    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is self attention?', role='user'),\n",
       " AIMessage(content='Self-attention is an attention mechanism that relates different positions of a single sequence to compute a representation of the sequence. It has been used successfully in various tasks like reading comprehension, abstractive summarization, and learning task-independent sentence representations. The Transformer model is the first transduction model that relies entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. Self-attention allows the model to draw global dependencies between input and output, enabling more parallelization and achieving state-of-the-art translation quality. In self-attention, all positions are connected with a constant number of sequentially executed operations, making it faster than recurrent layers for smaller sequence lengths. Self-attention can be restricted to considering only a neighborhood of a certain size to improve computational performance for tasks involving very long sequences. Multi-head attention, a variant of self-attention, linearly projects queries, keys, and values multiple times to attend to information from different representation subspaces at different positions. The Transformer model uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, allowing for efficient parallelization and improved performance.', role='assistant'),\n",
       " HumanMessage(content='What is my previous question?', role='user'),\n",
       " AIMessage(content='Your previous question was \"What is self attention?\"', role='assistant')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
